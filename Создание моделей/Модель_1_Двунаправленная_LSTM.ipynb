{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Модель 1. Двунаправленная LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN8VAdvqJC6R"
      },
      "source": [
        "# EmoSense at SemEval-2018 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOuc4dA-JC6a"
      },
      "source": [
        "## 1. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwpLuarVJC6b"
      },
      "source": [
        "!pip install ekphrasis\n",
        "#!pip install keras-utilities\n",
        "#!pip install matplotlib-venn\n",
        "!pip install tensorflow==2.4\n",
        "#!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzPeXHjv9LAT"
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import io\n",
        "\n",
        "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
        "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
        "\n",
        "emoticons_additional = {\n",
        "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
        "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
        "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
        "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
        "}\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "               'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "              'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\",\n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=True,  # spell correction for elongated words\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons, emoticons_additional]\n",
        ")\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    text = \" \".join(text_processor.pre_process_doc(text))\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessData(dataFilePath, mode):\n",
        "    conversations = []\n",
        "    labels = []\n",
        "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
        "        finput.readline()\n",
        "        for line in finput:\n",
        "            line = line.strip().split('\\t')\n",
        "            for i in range(1, 4):\n",
        "                line[i] = tokenize(line[i])\n",
        "            if mode == \"train\":\n",
        "                labels.append(emotion2label[line[4]])\n",
        "            conv = line[1:4]\n",
        "            conversations.append(conv)\n",
        "    if mode == \"train\":\n",
        "        return np.array(conversations), np.array(labels)\n",
        "    else:\n",
        "        return np.array(conversations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0ZwpSKZV00",
        "outputId": "a573a1de-aa56-474e-b215-eceaf8de1e9f"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY_1b0XcJi1Z",
        "outputId": "4e219add-c973-4a78-8179-9926d79b400d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y0KXqwSJluA",
        "outputId": "6b91372c-6159-4e58-b7aa-910c815927c4"
      },
      "source": [
        "!ls \"/content/drive/MyDrive/1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.txt\t\t      models\t    results.txt\t\t   train.csv\n",
            "devwithoutlabels.txt  results2.txt  test.txt\t\t   train.gsheet\n",
            "emosense.300d.txt     results3.txt  testwithoutlabels.txt  train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_o87WKNJC6e"
      },
      "source": [
        "texts_train, labels_train = preprocessData('/content/drive/MyDrive/1/train.txt', mode=\"train\")\n",
        "texts_dev, labels_dev = preprocessData('/content/drive/MyDrive/1/dev.txt', mode=\"train\")\n",
        "texts_test, labels_test = preprocessData('/content/drive/MyDrive/1/test.txt', mode=\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCw8OvQUJC6f"
      },
      "source": [
        "## 2. Loading Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj4AP80VJC6f"
      },
      "source": [
        "def getEmbeddings(file):\n",
        "    embeddingsIndex = {}\n",
        "    dim = 0\n",
        "    with io.open(file, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddingsIndex[word] = embeddingVector \n",
        "            dim = len(embeddingVector)\n",
        "    return embeddingsIndex, dim\n",
        "\n",
        "\n",
        "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
        "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
        "    for word, i in wordIndex.items():\n",
        "        embeddingMatrix[i] = embeddings.get(word)\n",
        "    return embeddingMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5diLsB9iLJc"
      },
      "source": [
        "#from tensorflow import keras\n",
        "#!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj3dYXBIJC6g",
        "scrolled": true
      },
      "source": [
        "#!pip install matplotlib-venn\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "embeddings, dim = getEmbeddings('/content/drive/MyDrive/1/emosense.300d.txt')\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
        "\n",
        "wordIndex = tokenizer.word_index\n",
        "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
        "\n",
        "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckOlamBJC6g"
      },
      "source": [
        "## 3. Texts Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_A4l-GZJC6g"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 24\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
        "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
        "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
        "labels_categorical_test = to_categorical(np.asarray(labels_test))\n",
        "\n",
        "\n",
        "def get_sequances(texts, sequence_length):\n",
        "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
        "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
        "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
        "    return message_first, message_second, message_third\n",
        "\n",
        "\n",
        "message_first_message_train, message_second_message_train, message_third_message_train = get_sequances(X_train, MAX_SEQUENCE_LENGTH)\n",
        "message_first_message_val, message_second_message_val, message_third_message_val = get_sequances(X_val, MAX_SEQUENCE_LENGTH)\n",
        "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequances(texts_dev, MAX_SEQUENCE_LENGTH)\n",
        "message_first_message_test, message_second_message_test, message_third_message_test = get_sequances(texts_test, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwVnHzfWEBqT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--RmYdUAJC6i"
      },
      "source": [
        "## 3. Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLWWVTZqJC6i"
      },
      "source": [
        "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
        "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
        "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
        "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
        "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
        "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
        "    embedding_dim = embeddings_matrix.shape[1]\n",
        "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
        "                                embedding_dim,\n",
        "                                weights=[embeddings_matrix],\n",
        "                                input_length=sequence_length,\n",
        "                                trainable=False)\n",
        "    \n",
        "    turn1_branch = embeddingLayer(turn1_input)\n",
        "    turn2_branch = embeddingLayer(turn2_input) \n",
        "    turn3_branch = embeddingLayer(turn3_input) \n",
        "    \n",
        "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
        "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
        "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
        "\n",
        "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
        "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
        "    \n",
        "    turn1_branch = lstm1(turn1_branch)\n",
        "    turn2_branch = lstm2(turn2_branch)\n",
        "    turn3_branch = lstm1(turn3_branch)\n",
        "    \n",
        "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
        "    \n",
        "    x = Dropout(dropout)(x)\n",
        "    \n",
        "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
        "    \n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Jjwf4QJC6j"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuasGx2YJC6k"
      },
      "source": [
        "!pip install matplotlib-venn\n",
        "#!pip install tensorflow==2.2\n",
        "!pip install keras==2.2.2\n",
        "!pip install keras-utilities\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "import matplotlib\n",
        "matplotlib.use('TKAgg')\n",
        "\n",
        "from kutilities.callbacks import MetricsCallback, PlottingCallback\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "\n",
        "metrics = {\n",
        "    \"f1_e\": (lambda y_test, y_pred:\n",
        "             f1_score(y_test, y_pred, average='micro',\n",
        "                      labels=[emotion2label['happy'],\n",
        "                              emotion2label['sad'],\n",
        "                              emotion2label['angry']\n",
        "                              ])),\n",
        "    \"precision_e\": (lambda y_test, y_pred:\n",
        "                    precision_score(y_test, y_pred, average='micro',\n",
        "                                    labels=[emotion2label['happy'],\n",
        "                                            emotion2label['sad'],\n",
        "                                            emotion2label['angry']\n",
        "                                            ])),\n",
        "    \"recoll_e\": (lambda y_test, y_pred:\n",
        "                 recall_score(y_test, y_pred, average='micro',\n",
        "                              labels=[emotion2label['happy'],\n",
        "                                      emotion2label['sad'],\n",
        "                                      emotion2label['angry']\n",
        "                                      ]))\n",
        "}\n",
        "\n",
        "_datasets = {}\n",
        "_datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n",
        "                    np.array(labels_categorical_dev)]\n",
        "_datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
        "                    np.array(labels_categorical_val)]\n",
        "\n",
        "metrics_callback = MetricsCallback(datasets=_datasets, metrics=metrics)\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/1/models/bidirectional_LSTM_best_weights_{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, save_weights_only=False,\n",
        "                             mode='auto', period=1)\n",
        "tensorboardCallback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrQaAIhnJC6l",
        "scrolled": false
      },
      "source": [
        "history = model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
        "                    np.array(labels_categorical_train),\n",
        "                    callbacks=[metrics_callback, checkpoint, tensorboardCallback],\n",
        "                    validation_data=(\n",
        "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
        "                        np.array(labels_categorical_val)\n",
        "                    ),\n",
        "                    epochs=20,\n",
        "                    batch_size=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4qrc7PzJC6m"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/1/models/bidirectional_LSTM_best_weights_04-0.9000.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai6wz9EPJC6m"
      },
      "source": [
        "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQbSU29JC6m"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for title, metric in metrics.items():\n",
        "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
        "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKgtyVDTJC6n"
      },
      "source": [
        "## 4. Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeBBP-cOJC6n"
      },
      "source": [
        "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
        "\n",
        "for title, metric in metrics.items():\n",
        "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
        "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}